{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f93f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'property_name': property_name,\n",
    "# 'link': link,\n",
    "# 'society': society,\n",
    "# 'price': price,\n",
    "# 'area': area,\n",
    "# 'areaWithType': areaWithType,\n",
    "#  'bedRoom': bedRoom,\n",
    "# 'bathroom': bathroom,\n",
    "# 'balcony': balcony,\n",
    "# 'additionalRoom': additionalRoom,\n",
    "#  'address': address,\n",
    "# 'floorNum': floorNum,\n",
    "# 'facing': facing,\n",
    "# 'agePossession': agePossession,\n",
    "# 'nearbyLocations': nearbyLocations,\n",
    "#  'description': description,\n",
    "#  'furnishDetails': furnishDetails,\n",
    "# 'features': features,\n",
    "# 'rating': rating,\n",
    "# 'property_id': property_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b1489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c5f7d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom=2,3&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment,Residential-House,Villa&cityName=Chandigarh'\n",
    "\n",
    "# #1-bhk data\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "\n",
    "#2bhk Data\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "\n",
    "url='https://www.99acres.com/flats-in-mumbai-ffid-page-11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4d7346",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage=requests.get(url).text\n",
    "webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fba6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.99acres.com/flats-in-mumbai-ffid-page-11'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    # Now, you can use BeautifulSoup to parse the HTML content\n",
    "    # ...\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067d117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming you have the Selenium webdriver installed and configured\n",
    "# driver = webdriver.Chrome()  # Or use another driver based on your browser\n",
    "\n",
    "# Replace 'url' with the actual URL of the website you are scraping\n",
    "# url='https://www.magicbricks.com/property-for-sale/commercial-real-estate?&cityName=Chandigarh&language=en&proptype=Commercial-Office-Space,Office-ITPark-SEZ'\n",
    "\n",
    "#1-bhk data\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "\n",
    "#2bhk Data\n",
    "# url='https://www.magicbricks.com/property-for-sale/residential-real-estate?&cityName=Chandigarh&Locality=Sector-34&language=en&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment'\n",
    "# Create a BeautifulSoup object with the retrieved HTML\n",
    "soup = BeautifulSoup(webpage)\n",
    "\n",
    "# Now you can find all 'h2' elements\n",
    "h2_elements = soup.find_all(class_=\"mb-srp__card--title\")\n",
    "\n",
    "# Print the text content of each 'h2' element\n",
    "for index, h2 in enumerate(h2_elements, start=1):\n",
    "    print(f\"Index {index}: {h2.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming you have the Selenium webdriver installed and configured\n",
    "driver = webdriver.Chrome()  # Or use another driver based on your browser\n",
    "\n",
    "# Replace 'url' with the actual URL of the website you are scraping\n",
    "url = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom=2,3&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment,Residential-House,Villa&cityName=Chandigarh'\n",
    "driver.get(url)\n",
    "\n",
    "# Use WebDriverWait to wait for the dynamic content to load\n",
    "wait = WebDriverWait(driver, 10)  # Adjust the time based on your website's loading time\n",
    "\n",
    "# Wait for the 'h2' elements to be present in the DOM\n",
    "h2_elements = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'h2.mb-srp__card--title')))\n",
    "\n",
    "# Get the HTML content after the dynamic content has loaded\n",
    "webpage = driver.page_source\n",
    "\n",
    "# Close the Selenium webdriver\n",
    "driver.quit()\n",
    "\n",
    "# Create a BeautifulSoup object with the retrieved HTML\n",
    "soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "# Now you can find all 'h2' elements\n",
    "for index, h2 in enumerate(h2_elements, start=1):\n",
    "    print(f\"Index {index}: {h2.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "781a8222",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://www.magicbricks.com/property-for-sale/commercial-real-estate?&cityName=Chandigarh&language=en&proptype=Commercial-Office-Space,Office-ITPark-SEZ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c663176",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage=requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1c19a671",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Response' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(webpage, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Now you can find all 'h2' elements\u001b[39;00m\n\u001b[0;32m      4\u001b[0m h2_elements \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh2\u001b[39m\u001b[38;5;124m'\u001b[39m,class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmb-srp__card__info mb-srp__card__info-withoutburger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\bs4\\__init__.py:315\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[1;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(markup, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):        \u001b[38;5;66;03m# It's a file-type object.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     markup \u001b[38;5;241m=\u001b[39m markup\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(markup) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    316\u001b[0m         (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    317\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(markup, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m markup)\n\u001b[0;32m    318\u001b[0m ):\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;66;03m# Issue warnings for a couple beginner problems\u001b[39;00m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;66;03m# involving passing non-markup to Beautiful Soup.\u001b[39;00m\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Beautiful Soup will still parse the input as markup,\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;66;03m# since that is sometimes the intended behavior.\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_is_url(markup):\n\u001b[0;32m    324\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markup_resembles_filename(markup)                \n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Response' has no len()"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "# Now you can find all 'h2' elements\n",
    "h2_elements = soup.find_all('h2',class_=\"mb-srp__card__info mb-srp__card__info-withoutburger\")\n",
    "\n",
    "# Print the text content of each 'h2' element\n",
    "for index, h2 in enumerate(h2_elements, start=1):\n",
    "    print(f\"Index {index}: {h2.text}\")\n",
    "\n",
    "h2_elements.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8351cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to change as per your requirement - city name\n",
    "# Match with 99acers site like for chandighars flats data site is : https://www.99acres.com/flats-in-chandigarh-ffid\n",
    "# Taking value of city as 'chandigarh'\n",
    "City = 'chandigarh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a36fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': f'https://www.99acres.com/flats-in-{City}-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17d251fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data\n",
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh\n",
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Flats\n",
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Societies\n",
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Residential\n",
      "Directory already exists: /content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Independent House\n"
     ]
    }
   ],
   "source": [
    "# If folder structures are in already created no need to run it.\n",
    "\n",
    "import os\n",
    "\n",
    "# Define the path to your project directory\n",
    "project_dir = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/'\n",
    "\n",
    "# Define the subdirectories\n",
    "subdirectories = ['Data', f'Data/{City}', f'Data/{City}/Flats', f'Data/{City}/Societies', f'Data/{City}/Residential', f'Data/{City}/Independent House']\n",
    "\n",
    "# Create the directory structure\n",
    "for subdir in subdirectories:\n",
    "    dir_path = os.path.join(project_dir, subdir)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "# Now, your directory structure is created./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042be185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter page number where you got error in last run.\n",
      "Enter page number to start from:10\n"
     ]
    }
   ],
   "source": [
    "start = int(input(\"Enter page number where you got error in last run.\\nEnter page number to start from:\")) # Starting Page\n",
    "\n",
    "# End Page number- you can change is for start i am taking 10pages at a time,\n",
    "# as IPs are gettig block after some time\n",
    "end = start+10\n",
    "\n",
    "pageNumber = start\n",
    "req=0\n",
    "\n",
    "flats = pd.DataFrame()\n",
    "\n",
    "try :\n",
    "    while pageNumber < end:\n",
    "        i=1\n",
    "        url = f'https://www.99acres.com/flats-in-{City}-ffid-page-{pageNumber}'\n",
    "        page = requests.get(url, headers=headers)\n",
    "        pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "        req+=1\n",
    "        for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
    "\n",
    "        # Extract property name and property sub-name\n",
    "            try:\n",
    "                property_name = soup.select_one('a.srpTuple__propertyName').text.strip()\n",
    "                # Extract link\n",
    "                link = soup.select_one('a.srpTuple__propertyName')['href']\n",
    "                society = soup.select_one('#srp_tuple_society_heading').text.strip()\n",
    "            except:\n",
    "                continue\n",
    "            # Detail Page\n",
    "            page = requests.get(link, headers=headers)\n",
    "            dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "            req += 1\n",
    "            try:\n",
    "                #price Range\n",
    "                price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "            except:\n",
    "                price = ''\n",
    "\n",
    "            # Area\n",
    "            try:\n",
    "                area = soup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "            except:\n",
    "                area =''\n",
    "            # Area with Type\n",
    "            try:\n",
    "                areaWithType = dpageSoup.select_one('#factArea').text.strip()\n",
    "            except:\n",
    "                areaWithType = ''\n",
    "\n",
    "\n",
    "            # Configuration\n",
    "            try:\n",
    "                bedRoom = dpageSoup.select_one('#bedRoomNum').text.strip()\n",
    "            except:\n",
    "                bedRoom = ''\n",
    "            try:\n",
    "                bathroom = dpageSoup.select_one('#bathroomNum').text.strip()\n",
    "            except:\n",
    "                bathroom = ''\n",
    "            try:\n",
    "                balcony = dpageSoup.select_one('#balconyNum').text.strip()\n",
    "            except:\n",
    "                balcony = ''\n",
    "\n",
    "            try:\n",
    "                additionalRoom = dpageSoup.select_one('#additionalRooms').text.strip()\n",
    "            except:\n",
    "                additionalRoom = ''\n",
    "\n",
    "\n",
    "            # Address\n",
    "\n",
    "            try:\n",
    "                address = dpageSoup.select_one('#address').text.strip()\n",
    "            except:\n",
    "                address = ''\n",
    "            # Floor Number\n",
    "            try:\n",
    "                floorNum = dpageSoup.select_one('#floorNumLabel').text.strip()\n",
    "            except:\n",
    "                floorNum = ''\n",
    "\n",
    "            try:\n",
    "                facing = dpageSoup.select_one('#facingLabel').text.strip()\n",
    "            except:\n",
    "                facing = ''\n",
    "\n",
    "            try:\n",
    "                agePossession = dpageSoup.select_one('#agePossessionLbl').text.strip()\n",
    "            except:\n",
    "                agePossession = ''\n",
    "\n",
    "            # Nearby Locations\n",
    "\n",
    "            try:\n",
    "                nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.NearByLocation__tagWrap').select('span.NearByLocation__infoText')]\n",
    "            except:\n",
    "                nearbyLocations = ''\n",
    "\n",
    "            # Descriptions\n",
    "            try:\n",
    "                description = dpageSoup.select_one('#description').text.strip()\n",
    "            except:\n",
    "                description = ''\n",
    "\n",
    "            # Furnish Details\n",
    "            try:\n",
    "                furnishDetails = [i.text.strip() for i in dpageSoup.select_one('#FurnishDetails').select('li')]\n",
    "            except:\n",
    "                furnishDetails = ''\n",
    "\n",
    "            # Features\n",
    "            if furnishDetails:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[1].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "            else:\n",
    "                try:\n",
    "                    features = [i.text.strip() for i in dpageSoup.select('#features')[0].select('li')]\n",
    "                except:\n",
    "                    features = ''\n",
    "\n",
    "\n",
    "\n",
    "            # Rating by Features\n",
    "            try:\n",
    "                rating = [i.text for i in dpageSoup.select_one('div.review__rightSide>div>ul>li>div').select('div.ratingByFeature__circleWrap')]\n",
    "            except:\n",
    "                rating = ''\n",
    "            # print(top_f)\n",
    "\n",
    "            try:\n",
    "                # Property ID\n",
    "                property_id = dpageSoup.select_one('#Prop_Id').text.strip()\n",
    "            except:\n",
    "                property_id = ''\n",
    "\n",
    "            # Create a dictionary with the given variables\n",
    "            property_data = {\n",
    "            'property_name': property_name,\n",
    "            'link': link,\n",
    "            'society': society,\n",
    "            'price': price,\n",
    "            'area': area,\n",
    "            'areaWithType': areaWithType,\n",
    "            'bedRoom': bedRoom,\n",
    "            'bathroom': bathroom,\n",
    "            'balcony': balcony,\n",
    "            'additionalRoom': additionalRoom,\n",
    "            'address': address,\n",
    "            'floorNum': floorNum,\n",
    "            'facing': facing,\n",
    "            'agePossession': agePossession,\n",
    "            'nearbyLocations': nearbyLocations,\n",
    "            'description': description,\n",
    "            'furnishDetails': furnishDetails,\n",
    "            'features': features,\n",
    "            'rating': rating,\n",
    "            'property_id': property_id\n",
    "        }\n",
    "\n",
    "\n",
    "            temp_df = pd.DataFrame.from_records([property_data])\n",
    "            # print(temp_df)\n",
    "            flats = pd.concat([flats, temp_df], ignore_index=True)\n",
    "            i += 1\n",
    "            # if os.path.isfile(csv_file):\n",
    "            # # Append DataFrame to the existing file without header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "            # else:\n",
    "            #     # Write DataFrame to the file with header\n",
    "            #     temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "\n",
    "            if req % 4==0:\n",
    "                time.sleep(10)\n",
    "            if req % 15 == 0:\n",
    "                time.sleep(50)\n",
    "        print(f'{pageNumber} -> {i}')\n",
    "        pageNumber += 1\n",
    "\n",
    "except AttributeError as e:\n",
    "    print(e)\n",
    "    print(\"----------------\")\n",
    "    print(\"\"\"Your IP might have blocked. Delete Runitme and reconnect again with updating start page number.\\n\n",
    "            You would see in output above like 1 -> 15\\ and so 1 is page number and 15 is data items extracted.\"\"\")\n",
    "    csv_file_path = f\"/content/drive/MyDrive/DSMP/Case Studies/Real estate/Data/chandigarh/Flats/flats_{City}_data-page-{start}-{pageNumber-1}.csv\"\n",
    "\n",
    "    # This file will be new every time if start page will chnage, but still taking here mode as append\n",
    "    if os.path.isfile(csv_file_path):\n",
    "    # Append DataFrame to the existing file without header\n",
    "        flats.to_csv(csv_file_path, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # Write DataFrame to the file with header - first time write\n",
    "        flats.to_csv(csv_file_path, mode='a', header=True, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79405f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files(folder_path, combined_file_path):\n",
    "    combined_data = pd.DataFrame()  # Create an empty DataFrame to hold the combined data\n",
    "\n",
    "    # Iterate through all CSV files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print('file_path')\n",
    "            # Read the data from the current CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Append the data to the combined DataFrame\n",
    "            combined_data = combined_data.append(df, ignore_index=True)\n",
    "\n",
    "            # Delete the original CSV file\n",
    "            os.remove(file_path)\n",
    "\n",
    "    # Save the combined data to a new CSV file\n",
    "    combined_data.to_csv(combined_file_path, index=False)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Replace with the actual folder path\n",
    "folder_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment'\n",
    "\n",
    "# Replace with the desired combined file path\n",
    "combined_file_path = '/content/drive/MyDrive/DSMP/Case Studies/Real estate/flats_appartment/flats.csv'\n",
    "\n",
    "combine_csv_files(folder_path, combined_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cc5d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(combined_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f85e356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "# User Agent\n",
    "# Headers set like below:\n",
    "# User Agent\n",
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://www.99acres.com/independent-house-in-gurgaon-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "396844e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_house = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2976a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Put start page number and end page number\n",
    "start = 1 # Starting Page\n",
    "end = 50 # End Page\n",
    "\n",
    "csv_file = f\"/content/drive/MyDrive/DSMP/Case Studies/Real estate/indpendent_house/independent_house-p-{start}-{end}.csv\"\n",
    "pageNumber = start\n",
    "req=0\n",
    "while pageNumber <= end:\n",
    "    i=1\n",
    "    url = f'https://www.99acres.com/independent-house-in-gurgaon-ffid-page-{pageNumber}'\n",
    "    page = requests.get(url, headers=headers)\n",
    "    pageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "    req+=1\n",
    "    for soup in pageSoup.select_one('div[data-label=\"SEARCH\"]').select('section[data-hydration-on-demand=\"true\"]'):\n",
    "\n",
    "    # Extract property name and property sub-name\n",
    "        try:\n",
    "            property_name = soup.select_one('a.srpTuple__propertyName').text.strip()\n",
    "            # Extract link\n",
    "            link = soup.select_one('a.srpTuple__propertyName')['href']\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        #Society\n",
    "        try:\n",
    "            society = soup.select_one('#srp_tuple_society_heading').text.strip()\n",
    "        except:\n",
    "            society=''\n",
    "\n",
    "        # Detail Page\n",
    "        time.sleep(1)\n",
    "        page = requests.get(link, headers=headers)\n",
    "        dpageSoup = BeautifulSoup(page.content, 'html.parser')\n",
    "        req+=1\n",
    "        try:\n",
    "            #price Range\n",
    "            price = dpageSoup.select_one('#pdPrice2').text.strip()\n",
    "        except:\n",
    "            price = ''\n",
    "\n",
    "        # Area\n",
    "        try:\n",
    "            ratePerArea = soup.select_one('#srp_tuple_price_per_unit_area').text.strip()\n",
    "        except:\n",
    "            ratePerArea =''\n",
    "        # Area with Type\n",
    "        try:\n",
    "            areaWithType = dpageSoup.select_one('#factArea').text.strip()\n",
    "        except:\n",
    "            areaWithType = ''\n",
    "        try:\n",
    "            area = soup.select_one('#srp_tuple_secondary_area').text.strip()\n",
    "        except:\n",
    "            area = ''\n",
    "\n",
    "        # Configuration\n",
    "        try:\n",
    "            bedRoom = dpageSoup.select_one('#bedRoomNum').text.strip()\n",
    "        except:\n",
    "            bedRoom = ''\n",
    "        try:\n",
    "            bathroom = dpageSoup.select_one('#bathroomNum').text.strip()\n",
    "        except:\n",
    "            bathroom = ''\n",
    "        try:\n",
    "            balcony = dpageSoup.select_one('#balconyNum').text.strip()\n",
    "        except:\n",
    "            balcony = ''\n",
    "\n",
    "        try:\n",
    "            additionalRoom = dpageSoup.select_one('#additionalRooms').text.strip()\n",
    "        except:\n",
    "            additionalRoom = ''\n",
    "\n",
    "\n",
    "        # Address\n",
    "\n",
    "        try:\n",
    "            address = dpageSoup.select_one('#address').text.strip()\n",
    "        except:\n",
    "            address = ''\n",
    "        # Floor Number\n",
    "        try:\n",
    "            noOfFloor = dpageSoup.select_one('#floorNumLabel').text.strip()\n",
    "        except:\n",
    "            noOfFloor = ''\n",
    "\n",
    "        try:\n",
    "            facing = dpageSoup.select_one('#facingLabel').text.strip()\n",
    "        except:\n",
    "            facing = ''\n",
    "\n",
    "        try:\n",
    "            agePossession = dpageSoup.select_one('#agePossessionLbl').text.strip()\n",
    "        except:\n",
    "            agePossession = ''\n",
    "\n",
    "        # Nearby Locations\n",
    "\n",
    "        try:\n",
    "            nearbyLocations = [i.text.strip() for i in dpageSoup.select_one('div.NearByLocation__tagWrap').select('span.NearByLocation__infoText')]\n",
    "        except:\n",
    "            nearbyLocations = ''\n",
    "\n",
    "        # Descriptions\n",
    "        try:\n",
    "            description = dpageSoup.select_one('#description').text.strip()\n",
    "        except:\n",
    "            description = ''\n",
    "\n",
    "        # Furnish Details\n",
    "        try:\n",
    "            furnishDetails = [i.text.strip() for i in dpageSoup.select_one('#FurnishDetails').select('li')]\n",
    "        except:\n",
    "            furnishDetails = ''\n",
    "\n",
    "        # Features\n",
    "        if furnishDetails:\n",
    "            try:\n",
    "                features = [i.text.strip() for i in dpageSoup.select('#features')[1].select('li')]\n",
    "            except:\n",
    "                features = ''\n",
    "        else:\n",
    "            try:\n",
    "                features = [i.text.strip() for i in dpageSoup.select('#features')[0].select('li')]\n",
    "            except:\n",
    "                features = ''\n",
    "\n",
    "\n",
    "\n",
    "        # Rating by Features\n",
    "        try:\n",
    "            rating = [i.text for i in dpageSoup.select_one('div.review__rightSide>div>ul>li>div').select('div.ratingByFeature__circleWrap')]\n",
    "        except:\n",
    "            rating = ''\n",
    "        # print(top_f)\n",
    "\n",
    "        try:\n",
    "        # Property ID\n",
    "            property_id = dpageSoup.select_one('#Prop_Id').text.strip()\n",
    "        except:\n",
    "            property_id = ''\n",
    "\n",
    "        # Create a dictionary with the given variables\n",
    "        property_data = {\n",
    "        'property_name': property_name,\n",
    "        'link': link,\n",
    "        'society': society,\n",
    "        'price': price,\n",
    "        'rate' : ratePerArea,\n",
    "        'area': area,\n",
    "        'areaWithType': areaWithType,\n",
    "        'bedRoom': bedRoom,\n",
    "        'bathroom': bathroom,\n",
    "        'balcony': balcony,\n",
    "        'additionalRoom': additionalRoom,\n",
    "        'address': address,\n",
    "        'noOfFloor': noOfFloor,\n",
    "        'facing': facing,\n",
    "        'agePossession': agePossession,\n",
    "        'nearbyLocations': nearbyLocations,\n",
    "        'description': description,\n",
    "        'furnishDetails': furnishDetails,\n",
    "        'features': features,\n",
    "        'rating': rating,\n",
    "        'property_id': property_id\n",
    "    }\n",
    "\n",
    "\n",
    "        temp_df = pd.DataFrame.from_records([property_data])\n",
    "        # print(temp_df)\n",
    "        ind_house = pd.concat([ind_house, temp_df], ignore_index=True)\n",
    "        i += 1\n",
    "\n",
    "        if os.path.isfile(csv_file):\n",
    "        # Append DataFrame to the existing file without header\n",
    "            temp_df.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "        else:\n",
    "            # Write DataFrame to the file with header\n",
    "            temp_df.to_csv(csv_file, mode='a', header=True, index=False)\n",
    "    print(f'{pageNumber} -> {i}')\n",
    "    pageNumber += 1\n",
    "    time.sleep(1)\n",
    "    if req%4==0:\n",
    "        time.sleep(10)\n",
    "    if req%20==0:\n",
    "        time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4665587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://drive.google.com/drive/folders/13NAbe8UCqBDfU1g0IhTDskL42N1VMN2N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca60dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Replace 'your_file_path.csv' with the actual path to your CSV file\n",
    "file_path = 'https://drive.google.com/drive/folders/13NAbe8UCqBDfU1g0IhTDskL42N1VMN2N?usp=drive_link'\n",
    "\n",
    "# Read the data into a Pandas DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Now, you can work with your data using the 'data' DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f78d62e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Assuming 'data' is the DataFrame containing your scraped data\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscraped_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# ... (your existing code)\n",
    "data='scraped_data.csv'\n",
    "# Assuming 'data' is the DataFrame containing your scraped data\n",
    "data.to_csv('scraped_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0547f95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "# User Agent\n",
    "# Headers set like below:\n",
    "# User Agent\n",
    "headers = {\n",
    "    'authority': 'www.99acres.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'no-cache',\n",
    "    'dnt': '1',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://www.99acres.com/independent-house-in-gurgaon-ffid-page',\n",
    "    'sec-ch-ua': '\"Chromium\";v=\"107\", \"Not;A=Brand\";v=\"8\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"macOS\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/527.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = 'https://www.99acres.com/flats-in-mumbai-ffid-page-11'\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    # Now, you can use BeautifulSoup to parse the HTML content\n",
    "    # ...\n",
    "else:\n",
    "    print(f\"Failed to fetch the page. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11cf5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
